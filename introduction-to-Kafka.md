# Introduction to Data Streaming and Apache Kafka

## **What is Streaming Data?**  
**Streaming data** (also called **event stream processing**) is the **continuous flow of data** generated by various sources, processed in **real-time** to extract insights and trigger actions.

## **Characteristics of Real-Time Data Processing**  
1. **Continuous flow** – Data is constantly generated with no "end."  
2. **Real-time processing** – Instant analysis for timely insights (no batch delays).  
3. **Event-driven architecture** – Systems react dynamically to individual events.  
4. **Scalability & fault tolerance** – Handles high traffic and recovers from failures.  
5. **Varied Data Sources** – Streaming data originates from sensors, logs, APIs, applications, mobile devices, and more.  

---

## **Key Benefits of Real-Time Data Processing**  
- **Immediate Insights**: Analyze data as it’s generated.  
- **Instant Decision-Making**: Respond to events in real-time (e.g., fraud detection).  
- **Operational Efficiency**: Optimize workflows and reduce downtime.  
- **Enhanced User Experience**: Personalize experiences using live data.  

### **Critical Use Cases**  
- **Fraud Detection**: Block suspicious transactions instantly.  
- **IoT Monitoring**: Track device health in real-time.  
- **Live Analytics**: Power dashboards with up-to-the-second data.  

### **Discussion Questions**  
1. Can you think of a real-time use case near you (e.g., mobile money, delivery apps)?  
2. What happens when systems can’t process data in real-time?  

---

# **Introduction to Apache Kafka**  

## **Key Concepts**  
- **Publish/Subscribe Model**:  
  - Producers send/write (**publish**) messages.  
  - Consumers receive/read (**subscribe**) messages.  
    > Asynchronous means not happening or done at the same time or speed — producer and consumer don’t need to wait on each other.

- **Common Use Cases**:  
  - **Real-time analytics** Analyze data as it's generated, instead of waiting for batch jobs or reports. 
  - **Log collection** Gather logs from multiple systems/services into a centralized location for monitoring, debugging, or auditing. 
  - **Event sourcing** Store state-changing events (like deposit $20, withdraw $30) rather than only storing the final state (balance = $50).

---

## **Kafka Architecture**  
| Component    | Role                                                                 |
|--------------|----------------------------------------------------------------------|
| **Producer** | Sends messages/events to a Kafka topic.                                     |
| **Consumer** | Reads messages/events from a topic.                                         |
| **Broker**   | Kafka server storing/serving messages (clusters = multiple brokers). |
| **ZooKeeper**/**KRaft** | Manages cluster state/metadata (KRaft replaces ZooKeeper in Kafka 4.0+). |

---
## **Event**:
- An event records the fact that "something happened" in the real world or in your system.
- It’s the fundamental unit of data in Kafka and may also be referred to as a record or message.
- Events are immutable — once written, they are not updated
- When you produce (write) or consume (read) data in Kafka, you're interacting with events.

#### Structure of an Event
An event in Kafka typically contains the following components:
  - Key: Identifies the event (e.g., the user, transaction ID, or source). Used for partitioning logic.
  - Value: The actual data or payload (e.g., what happened).
  - Timestamp: Time when the event occurred or was written.
  - Headers (optional): Metadata about the event, such as content type or correlation ID.

#### Example Event
```plaintext
Event key:    "Alice"
Event value:  "Made a payment of $200"
Timestamp:    2025-06-27T08:45:30Z
Headers:      { "source": "mobile-app", "transaction-id": "TXN-4490" }
```
This event could be sent to a topic like `payments` and later consumed by analytics or fraud detection services.

## **Topic**:  
- A topic is a category/feed name to which messages/events are published to.(similar to a database table).  
- Topics are split into **partitions** for scalability/parallel processing.  

#### **Examples**  
- `orders` – E-commerce purchases.  
- `user-logins` – Authentication events.  
- `click-events` – Website/app interactions.  

## **Partition**:  
  - A topic can be split into multiple partitions, which enables scalability (more messages) and parallelism (faster processing).
  -  Messages within a partition are strictly ordered.
    
## **Offset**:  
  - A unique identifier (number) that Kafka assigns to each message within a partition.  
  - It allows consumers to track where they left off in reading the stream of messages. (e.g., "read up to offset #5").















